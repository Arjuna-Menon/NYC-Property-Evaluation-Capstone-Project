{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from keras import backend as K\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from math import sqrt\n",
    "\n",
    "def generate_report(y_actual, y_pred):\n",
    "    mse = round(mean_squared_error(y_actual, y_pred),3)\n",
    "    rmse = round(sqrt(mean_squared_error(y_actual, y_pred)),3)\n",
    "    r2 = round(r2_score(y_actual, y_pred),3)\n",
    "    error = np.mean(pd.DataFrame(y_train) - pd.DataFrame(y_pred))[0]\n",
    "    print('mse',mse)\n",
    "    print('RMSE', rmse)\n",
    "    print('R2', r2)\n",
    "    print('error', error)\n",
    "    return mse,rmse,r2,error\n",
    "\n",
    "def generate_loss_plot(history, filename=None):\n",
    "    plt.plot(history.history['loss'])\n",
    "    plt.plot(history.history['val_loss'])\n",
    "    plt.title('loss curve')\n",
    "    plt.ylabel('loss')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'test'], loc='upper left')\n",
    "    if (filename!=None):\n",
    "        plt.savefig(filename)\n",
    "    plt.show()\n",
    "\n",
    "def generate_hist_plot(y_actual, y_pred, filename=None):\n",
    "    y = pd.DataFrame(y_actual)\n",
    "    y['new']=y.index\n",
    "    pred = pd.DataFrame(y_pred)\n",
    "    pred.index=y['new'].values\n",
    "    y = y.drop('new',axis=1)\n",
    "    pred = pred.rename(columns={0:'predicted'})\n",
    "    x =pd.DataFrame(y[0]-pred['predicted'])\n",
    "    x = x.rename(columns={0:'difference'})\n",
    "    done = pd.concat([x,y,pred],axis=1)\n",
    "    p = x['difference'].values\n",
    "    type(p)\n",
    "    plt.hist(p, bins='auto', range=(-75000, 75000))\n",
    "    if (filename!=None):\n",
    "        plt.savefig(filename)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def get_data(): \n",
    "    df = pd.read_csv('pluto5_stddum.csv')\n",
    "    df.drop(['assessland'], axis=1, inplace=True)\n",
    "    \n",
    "    X = df[df.columns]\n",
    "    X.drop('assesstot', axis=1, inplace=True)\n",
    "    predictors = X.columns\n",
    "    X = X.values\n",
    "    Y = df['assesstot'].values\n",
    "    x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size=0.2, random_state=0)\n",
    "    return x_train, x_test, y_train, y_test, predictors\n",
    "\n",
    "#3)Adam combines the good properties of Adadelta and RMSprop and hence tend to do better for most of the problems.\n",
    "def fit_model(model, x_train, x_test, y_train, y_test, optimizer, epochs):\n",
    "    model.compile(loss='mse', optimizer=optimizer, metrics=['mse'])\n",
    "    history = model.fit(x_train, y_train, epochs=epochs, verbose=0, validation_data=(x_test, y_test))\n",
    "    generate_loss_plot(history, filename=None)\n",
    "    return model\n",
    "\n",
    "def plot_comparation(y_test, y_test_pred, filename):\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.plot(y_test, color = 'blue')\n",
    "    ax.plot(y_test_pred, color = 'red')\n",
    "    ax.legend(['Real', 'Predicted'])\n",
    "    if (filename!=None):\n",
    "        fig.savefig(filename)\n",
    "    plt.show()\n",
    "\n",
    "def predict(model, x_train, y_train, x_test, y_test, filename=None):\n",
    "    y_train_pred = model.predict(x_train)\n",
    "    y_test_pred = model.predict(x_test)\n",
    "    print('ERROR Training')\n",
    "    generate_report(y_train, y_train_pred)\n",
    "    print('ERROR Test')\n",
    "    mse,rmse,r2,error = generate_report(y_test, y_test_pred)\n",
    "    print('Histogram Training')\n",
    "    generate_hist_plot(y_train, y_train_pred)\n",
    "    print('Histogram Test')\n",
    "    generate_hist_plot(y_test, y_test_pred)\n",
    "    return y_train_pred, y_test_pred, mse,rmse,r2,error\n",
    "    \n",
    "def run_model(hidden_nodes, x_train, x_test, y_train, y_test, optimizer, epochs):\n",
    "    model = tf.keras.models.Sequential()\n",
    "    model.add(tf.keras.layers.Dense(input_nodes, tf.keras.activations.linear))\n",
    "    model.add(tf.keras.layers.Dense(hidden_nodes, tf.keras.activations.relu))\n",
    "    model.add(tf.keras.layers.Dense(1, tf.keras.activations.linear))\n",
    "    model = fit_model(model, x_train, x_test, y_train, y_test, optimizer, epochs)\n",
    "    y_train_pred, y_test_pred, mse,rmse,r2,error = predict(model, x_train, y_train, x_test, y_test, filename=None)\n",
    "    plot_comparation(y_test, y_test_pred, filename=None)\n",
    "    return y_train_pred, y_test_pred, mse,rmse,r2,error\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test, predictors = get_data()\n",
    "input_nodes = len(predictors)\n",
    "epochs = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#choose the amount of nodes in hidden layers: http://www.faqs.org/faqs/ai-faq/neural-nets/part3/section-10.html\n",
    "#NN0: 1 hidden layer with (Number of inputs + outputs) * (2/3) nodes: overfitting\n",
    "#does not predicts well high values but it might be because sample \n",
    "print('Model 0')\n",
    "y_train_pred, y_test_pred, mse,rmse,r2,error = run_model(int((input_nodes+1)*(2/3)), x_train, x_test, y_train, y_test, 'adam', epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#NN1: # A typical recommendation is that the number of weights should be no more than 1/30 of the number of training cases: underfitting\n",
    "print('Model 1')\n",
    "y_train_pred, y_test_pred = run_model(int(len(x_train)/(30*2)), x_train, x_test, y_train, y_test, 'adam', epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#NN2: reduce amount of nodes hidden layer: underfitting\n",
    "print('Model 2')\n",
    "y_train_pred, y_test_pred = run_model(int(len(x_train)/(30*4)), x_train, x_test, y_train, y_test, 'adam', epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#NN3: reduce amount of nodes hidden layer: underfitting\n",
    "print('Model 3')\n",
    "y_train_pred, y_test_pred = run_model(int(len(x_train)/(30*6)), x_train, x_test, y_train, y_test,'adam', epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#NN4: reduce amount of nodes hidden layer: underfitting\n",
    "print('Model 4')\n",
    "y_train_pred, y_test_pred = run_model(int(len(x_train)/(30*8)), x_train, x_test, y_train, y_test, 'adam', epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#NN5: without hidden layer\n",
    "print('Model 5')\n",
    "model = tf.keras.models.Sequential()\n",
    "model.add(tf.keras.layers.Dense(input_nodes, tf.keras.activations.linear))\n",
    "model.add(tf.keras.layers.Dense(1, tf.keras.activations.linear))\n",
    "model = fit_model(model, x_train, x_test, y_train, y_test, epochs)\n",
    "model.compile(loss='mse', optimizer='adam', metrics=['mse'])\n",
    "history = model.fit(x_train, y_train, epochs=epochs, verbose=0, validation_data=(x_test, y_test))\n",
    "generate_loss_plot(history, filename=None)\n",
    "predict(model, x_train, y_train, x_test, y_test, filename=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ACTIVATION FUNCTION\n",
    "#what is it? Convert a input signal of a node in a A-NN to an output signal.\n",
    "#Decides, whether a neuron should be activated or not by calculating weighted sum and further adding bias with it. \n",
    "#The purpose of the activation function is to introduce non-linearity into the output of a neuron.\n",
    "\n",
    "#https://medium.com/the-theory-of-everything/understanding-activation-functions-in-neural-networks-9491262884e0\n",
    "#Sigmoid -> good for classifier \n",
    "#Tanh -> scaled sigmoid \n",
    "#Relu -> output 1 if greater than 0, makes the activations sparse and efficient. Good when you don’t know the nature of the function you are trying to learn. \n",
    "#But its limitation is that it should only be used within Hidden layers of a Neural Network Model.\n",
    "#softmax -> classifier with multiple classes\n",
    "\n",
    "#For prediction problem it should simply use a linear function for output layer, for classification sigmoid \n",
    "#The basic rule of thumb is if you really don’t know what activation function to use, then simply use RELU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#OPTIMIZERS\n",
    "#https://medium.com/datadriveninvestor/overview-of-different-optimizers-for-neural-networks-e0ed119440c3\n",
    "#https://www.dlology.com/blog/quick-notes-on-how-to-choose-optimizer-in-keras/\n",
    "\n",
    "#RMSprop: Root Mean Square Propagation. \n",
    "#It utilizes the magnitude of the recent gradient descents to normalize the gradient.\n",
    "#This optimizer is usually a good choice for recurrent neural networks.\n",
    "#learning rate gets adjusted automatically and it chooses a different learning rate for each parameter.\n",
    "#RMSProp divides the learning rate by the average of the exponential decay of squared gradients\n",
    "\n",
    "#Adam: Adam can be viewed as a combination of Adagrad, which works well on sparse gradients and \n",
    "#RMSprop which works well in online and nonstationary settings.\n",
    "\n",
    "#Stochastic gradient descent(SGD): for shallow networks.\n",
    "\n",
    "#Adagrad:  perform larger updates for infrequent parameters and smaller updates for frequent parameters.\n",
    "\n",
    "#AdaDelta: Adadelta is an extension of Adagrad and it also tries to reduce Adagrad’s aggressive, monotonically reducing the learning rate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
